# -*- coding: utf-8 -*-
"""
White Label Resume Builder:
1) build_white_label_prompt(...) ‚Äî –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT/Gemini (—Å—Ç—Ä–æ–≥–∏–µ –ø—Ä–∞–≤–∏–ª–∞, Projects ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ).
2) generate_resume_payload_gemini(...) ‚Äî –∑–∞–ø—Ä–æ—Å –∫ Gemini, –≤–æ–∑–≤—Ä–∞—Ç JSON payload {config, content}.
3) render_resume_docx(payload) ‚Äî —Ä–µ–Ω–¥–µ—Ä –∫—Ä–∞—Å–∏–≤–æ–≥–æ .docx –ø–æ JSON (Times New Roman, —Å–∏–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏).
4) create_white_label_resume(...) ‚Äî –ø–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä: –∫–∞–Ω–¥–∏–¥–∞—Ç—Å–∫–∏–π —Ç–µ–∫—Å—Ç -> JSON -> DOCX.
5) parse_json_loose(...) ‚Äî ¬´–∂–∏–≤—É—á–∏–π¬ª –ø–∞—Ä—Å–µ—Ä JSON –∏–∑ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏.
"""

from datetime import datetime
from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_BREAK
import json, re
from dotenv import load_dotenv
import os
import google.generativeai as genai
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()

def _extract_text_from_gemini_response(resp) -> str:
    try:
        if getattr(resp, "text", None):
            return (resp.text or "").strip()
    except Exception:
        pass
    out = []
    try:
        for cand in (getattr(resp, "candidates", None) or []):
            content = getattr(cand, "content", None)
            parts = getattr(content, "parts", None) if content else None
            if not parts:
                continue
            for p in parts:
                t = getattr(p, "text", None)
                if t: out.append(t)
    except Exception:
        pass
    return "\n".join(out).strip()

def _hex_to_rgb(hex_color: str) -> RGBColor:
    h = (hex_color or "#000000").lstrip('#')
    return RGBColor(int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16))

def _set_core_styles(doc: Document, font_family: str, font_size_main: int):
    style = doc.styles['Normal']
    style.font.name = font_family
    style.font.size = Pt(font_size_main)
    for sec in doc.sections:
        sec.top_margin = Inches(0.8)
        sec.bottom_margin = Inches(0.8)
        sec.left_margin = Inches(0.8)
        sec.right_margin = Inches(0.8)

def _add_section_title(doc: Document, title: str, color_hex: str, font_size_headings: int):
    p = doc.add_paragraph()
    r = p.add_run(title)
    r.bold = True
    r.font.size = Pt(font_size_headings)
    r.font.color.rgb = _hex_to_rgb(color_hex)
    p.paragraph_format.line_spacing = 1.2
    p.paragraph_format.space_before = Pt(6)
    p.paragraph_format.space_after = Pt(4)

def _add_text(doc: Document, text: str, bold: bool = False):
    if not text:
        return
    p = doc.add_paragraph(text)
    if p.runs:
        p.runs[0].bold = bool(bold)
    p.paragraph_format.line_spacing = 1.2
    p.paragraph_format.space_after = Pt(2)

def _render_skills(doc: Document, skills):
    if not skills:
        return
    if isinstance(skills, str):
        _add_text(doc, skills)
    elif isinstance(skills, list):
        _add_text(doc, ", ".join(map(str, skills)))
    elif isinstance(skills, dict):
        for k, v in skills.items():
            line = f"{k}: {', '.join(map(str, v)) if isinstance(v, list) else v}"
            _add_text(doc, line)

def _render_experience(doc: Document, exp_list: list):
    if not exp_list:
        return
    for it in exp_list:
        header = " ‚Äî ".join([x for x in [it.get("company"), it.get("position")] if x])
        if header: _add_text(doc, header, bold=True)
        if it.get("period"): _add_text(doc, it["period"], bold=True)  # –°–¥–µ–ª–∞—Ç—å –ø–µ—Ä–∏–æ–¥ –ø–æ–ª—É–∂–∏—Ä–Ω—ã–º
        for key in ("responsibilities", "achievements"):
            for ln in (it.get(key) or []):
                _add_text(doc, ln)
        techs = it.get("technologies") or []
        if techs:
            _add_text(doc, f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(map(str, techs))}")

def _render_education(doc: Document, education):
    if not education:
        return
    if isinstance(education, str):
        _add_text(doc, education); return
    for ed in education:
        line = " ‚Äî ".join(filter(None, [ed.get("institution"), ed.get("degree")]))
        if line: _add_text(doc, line, bold=True)
        if ed.get("years"): _add_text(doc, ed["years"])
        if ed.get("details"): _add_text(doc, ed["details"])

def _render_projects(doc: Document, projects: list):
    if not projects:
        return
    for pr in projects:
        if pr.get("title"): _add_text(doc, pr["title"], bold=True)
        if pr.get("role"): _add_text(doc, f"–†–æ–ª—å: {pr['role']}")
        if pr.get("period"): _add_text(doc, f"–ü–µ—Ä–∏–æ–¥: {pr['period']}")
        if pr.get("description"): _add_text(doc, pr["description"])
        techs = pr.get("technologies") or []
        if techs: _add_text(doc, f"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(map(str, techs))}")
        if pr.get("results"): _add_text(doc, f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {pr['results']}")

def _post_fix_bold_skills(doc: Document):
    SECTION_HEADERS = {
        "–†–ï–ó–Æ–ú–ï", "–ö–†–ê–¢–ö–û–ï –û–ü–ò–°–ê–ù–ò–ï –ü–†–û–§–ò–õ–Ø",
        "–ö–õ–Æ–ß–ï–í–´–ï –ù–ê–í–´–ö–ò", "–û–ü–´–¢ –†–ê–ë–û–¢–´", "–û–ë–†–ê–ó–û–í–ê–ù–ò–ï",
        "–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø", "–ü–†–û–ï–ö–¢–´",
    }
    start_idx, end_idx = None, None
    for i, p in enumerate(doc.paragraphs):
        if p.text.strip().upper() == "–ö–õ–Æ–ß–ï–í–´–ï –ù–ê–í–´–ö–ò":
            start_idx = i + 1
            break
    if start_idx is None:
        return
    for j in range(start_idx, len(doc.paragraphs)):
        if doc.paragraphs[j].text.strip().upper() in SECTION_HEADERS:
            end_idx = j
            break
    if end_idx is None:
        end_idx = len(doc.paragraphs)
    for k in range(start_idx, end_idx):
        p = doc.paragraphs[k]
        txt = p.text
        if not txt or ":" not in txt:
            continue
        idx = txt.find(":")
        head = txt[: idx + 1].strip()
        tail = txt[idx + 1 :].lstrip()
        for r in p.runs: r.text = ""
        run_head = p.add_run(head + (" " if tail else "")); run_head.bold = True
        if tail:
            run_tail = p.add_run(tail); run_tail.bold = False

def _post_fix_inline_dicts(doc: Document):
    pattern = re.compile(r"^\s*\{\s*'title'\s*:\s*'([^']*)'\s*,\s*'items'\s*:\s*\[([^\]]*)\]\s*\}\s*$")
    for p in doc.paragraphs:
        m = pattern.match(p.text.strip())
        if not m:
            continue
        title = m.group(1).strip()
        items_raw = m.group(2).strip()
        parts = [x.strip().strip("'").strip('"') for x in items_raw.split(",") if x.strip()]
        for r in p.runs: r.text = ""
        if title:
            rt = p.add_run(title); rt.bold = True
        for it in parts:
            br = p.add_run(); br.add_break(WD_BREAK.LINE)
            p.add_run(it)

# --- –ù–æ–≤–æ–µ: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ –ø–æ-—Ä—É—Å—Å–∫–∏ –∏ –≤—Å—Ç–∞–≤–∫–∞ —Ä–∞–∑–¥–µ–ª–∞ ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª ---

_MONTHS_RU_GEN = {
    1: "—è–Ω–≤–∞—Ä—è", 2: "—Ñ–µ–≤—Ä–∞–ª—è", 3: "–º–∞—Ä—Ç–∞", 4: "–∞–ø—Ä–µ–ª—è",
    5: "–º–∞—è", 6: "–∏—é–Ω—è", 7: "–∏—é–ª—è", 8: "–∞–≤–≥—É—Å—Ç–∞",
    9: "—Å–µ–Ω—Ç—è–±—Ä—è", 10: "–æ–∫—Ç—è–±—Ä—è", 11: "–Ω–æ—è–±—Ä—è", 12: "–¥–µ–∫–∞–±—Ä—è"
}

def _format_dt_ru(dt: datetime) -> str:
    """16 –∞–≤–≥—É—Å—Ç–∞ 2025 –≤ 11:31"""
    return f"{dt.day} {_MONTHS_RU_GEN[dt.month]} {dt.year} –≤ {dt.strftime('%H:%M')}"

def _render_primichanie(doc: Document, color_hex: str, font_size_headings: int, utochnenie):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª, –µ—Å–ª–∏ utochnenie –Ω–µ –ø—É—Å—Ç–æ.
       –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞: ¬´–†–µ–∑—é–º–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: <–¥–∞—Ç–∞> (–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞).¬ª
       –î–∞–ª–µ–µ: –ø–æ –æ–¥–Ω–æ–º—É ¬´–î–æ–±–∞–≤–ª–µ–Ω–æ: <—ç–ª–µ–º–µ–Ω—Ç>¬ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –∏–∑ utochnenie.
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º utochnenie –∫ —Å–ø–∏—Å–∫—É —Å—Ç—Ä–æ–∫
    items = []
    if isinstance(utochnenie, str):
        s = utochnenie.strip()
        if s:
            # –†–∞–∑—Ä–µ—à–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–µ —Å –∑–∞–ø—è—Ç–æ–π/–ø–µ—Ä–µ–≤–æ–¥–∞–º —Å—Ç—Ä–æ–∫/–∑–∞–ø—è—Ç—ã–º ‚Äî –Ω–æ –±–µ–∑ –≤—ã–∫–∏–¥—ã–≤–∞–Ω–∏—è —Å–º—ã—Å–ª–∞
            candidates = re.split(r"[;\n]+", s)
            for c in candidates:
                cc = c.strip().strip("-‚Äì‚Ä¢").strip()
                if cc:
                    items.append(cc)
    elif isinstance(utochnenie, (list, tuple, set)):
        for x in utochnenie:
            xx = (str(x) if not isinstance(x, str) else x).strip().strip("-‚Äì‚Ä¢").strip()
            if xx:
                items.append(xx)

    if not items:
        return

    _add_section_title(doc, "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ", color_hex, font_size_headings)
    _add_text(doc, f"–†–µ–∑—é–º–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {_format_dt_ru(datetime.now())} (–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞).")
    for it in items:
        _add_text(doc, f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {it}")

def render_resume_docx(payload: dict, vacancy_text: str = "", utochnenie=None, username = "") -> str:
    cfg = payload.get("config", {})
    cnt = payload.get("content", {})
    doc = Document()
    _set_core_styles(doc, cfg.get("font_family", "Times New Roman"), int(cfg.get("font_size_main", 12)))
    color = cfg.get("color_headings", "#1F4E79")
    hsize = int(cfg.get("font_size_headings", 14))
    sections = cfg.get("sections", [
        "–§–ò–û","–†–ï–ó–Æ–ú–ï","–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è",
        "–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏","–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã","–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ","–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è","–ü—Ä–æ–µ–∫—Ç—ã"
    ])
    fio = cnt.get("fio") or {}
    # –£–±–∏—Ä–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –§–ò–û –≤ –Ω–∞—á–∞–ª–µ - –æ–Ω–æ –±—É–¥–µ—Ç –≤ —Å–µ–∫—Ü–∏–∏ –†–ï–ó–Æ–ú–ï
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–±—Ä–∞–ª–∞ —Å–µ–∫—Ü–∏—é "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è" (–û —Å–µ–±–µ),
    # –Ω–æ –µ—Å—Ç—å –ø–æ–ª–µ extra –≤ content, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–∏–º —Å–µ–∫—Ü–∏—é,
    # —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ç–µ–∫—Å—Ç "–û —Å–µ–±–µ" –Ω–µ —Ç–µ—Ä—è–ª—Å—è.
    try:
        if cnt.get("extra"):
            upper_sections = [s.upper() for s in sections]
            if not any(s in ("–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø", "–û –°–ï–ë–ï") for s in upper_sections):
                # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–¥ '–ü–†–û–ï–ö–¢–´', –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –≤ –∫–æ–Ω–µ—Ü
                try:
                    idx = upper_sections.index("–ü–†–û–ï–ö–¢–´")
                except ValueError:
                    idx = len(sections)
                sections.insert(idx, "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
    except Exception:
        # –í —Å–ª—É—á–∞–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        pass
    
    for sec in sections:
        su = sec.upper()
        if su == "–§–ò–û":
            continue
        _add_section_title(doc, sec, color, hsize)
        if su == "–†–ï–ó–Æ–ú–ï":
            if fio.get("full_name"): _add_text(doc, f"–§–ò–û: {fio['full_name']}", bold=True)
            if cnt.get("position_grade"): _add_text(doc, f"–î–û–õ–ñ–ù–û–°–¢–¨: {cnt['position_grade']}", bold=True)
            if cnt.get("grade"): _add_text(doc, f"–ì—Ä–µ–π–¥: {cnt['grade']}", bold=True)
            if fio.get("location"): _add_text(doc, f"–õ–æ–∫–∞—Ü–∏—è: {fio['location']}", bold=True)
            if fio.get("citizenship"): _add_text(doc, f"–ì—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ: {fio['citizenship']}", bold=True)
            if fio.get("birth_date"): _add_text(doc, f"–î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è: {fio['birth_date']}", bold=True)
        elif su == "–ö–†–ê–¢–ö–û–ï –û–ü–ò–°–ê–ù–ò–ï –ü–†–û–§–ò–õ–Ø":
            if cnt.get("summary"): _add_text(doc, cnt["summary"])
        elif su == "–ö–õ–Æ–ß–ï–í–´–ï –ù–ê–í–´–ö–ò":
            _render_skills(doc, cnt.get("skills"))
        elif su == "–û–ü–´–¢ –†–ê–ë–û–¢–´":
            _render_experience(doc, cnt.get("experience"))
        elif su == "–û–ë–†–ê–ó–û–í–ê–ù–ò–ï":
            _render_education(doc, cnt.get("education"))
        elif su == "–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø":
            extra = cnt.get("extra")
            if isinstance(extra, list):
                combined_text = ", ".join(str(ln) for ln in extra if ln)
                if combined_text:
                    _add_text(doc, combined_text)
            elif isinstance(extra, str) and extra.strip():
                import re
                # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –∏ –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                cleaned = re.sub(r"\s*\n\s*", " ", extra.strip())
                cleaned = re.sub(r"\s{2,}", " ", cleaned)
                _add_text(doc, cleaned)
        elif su == "–ü–†–û–ï–ö–¢–´":
            _render_projects(doc, cnt.get("projects"))
    _post_fix_bold_skills(doc)
    _post_fix_inline_dicts(doc)
    
    # –í—ã–¥–µ–ª—è–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º
    if vacancy_text:
        technologies = _extract_technologies_from_vacancy(vacancy_text)
        _highlight_technologies_in_text(doc, technologies)

    # --- –ù–æ–≤–æ–µ: —Ä–∞–∑–¥–µ–ª ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª, –µ—Å–ª–∏ –µ—Å—Ç—å utochnenie ---
    _render_primichanie(doc, color, hsize, utochnenie)

    name_for_file = (cnt.get("fio") or {}).get("full_name") or "Name"
    date_str = datetime.now().strftime("%Y-%m-%d")
    # Use current directory instead of Linux path
    import os, re
    from urllib.parse import quote

    ILLEGAL_CHARS = re.compile(r'[\x00-\x1f<>:"/\\|?*]+')  # –∑–∞–ø—Ä–µ—Ç—ã Win/Unix

    def safe_unicode_name(s: str) -> str:
        s = (s or "").strip()
        s = s.replace(" ", "_")
        s = ILLEGAL_CHARS.sub("_", s)   # —É–±–∏—Ä–∞–µ–º —Å–ª—ç—à–∏, –¥–≤–æ–µ—Ç–æ—á–∏—è –∏ —Ç.–ø.
        s = s.strip("._-")              # –∫—Ä–∞—è
        return s or "file"

    # dir_path: –∫–∞—Ç–∞–ª–æ–≥, –≥–¥–µ —Ö—Ä–∞–Ω–∏–º WL
    # name_for_file: –§–ò–û/–Ω–∞–∑–≤–∞–Ω–∏–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
    # date_str: YYYY-MM-DD/ YYYYMMDD ‚Äî —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å
    # username: –ª–æ–≥–∏–Ω/–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    dir_path = os.path.abspath("WhiteLabel_Resume")
    os.makedirs(dir_path, exist_ok=True)
    filename = f"WhiteLabel_Resume_{safe_unicode_name(name_for_file)}_{date_str}_{safe_unicode_name(username)}.docx"
    wlfn = os.path.join(dir_path, filename)

    doc.save(wlfn)  # Unicode-–∏–º—è –Ω–∞ NTFS/EXT4 ‚Äî –æ–∫

    # –û—Ç–¥–∞—ë–º —Ñ—Ä–æ–Ω—Ç—É —Å—Å—ã–ª–∫—É —Å URL-—ç–Ω–∫–æ–¥–æ–º (–∏–Ω–∞—á–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –≤ href —Å–ª–æ–º–∞–µ—Ç –º–∞—Ä—à—Ä—É—Ç)
    download_link = f"/api/wl/download/{quote(filename)}"
    return {"download_link": download_link, "filename": filename}

# --------- –î–û–ë–ê–í–õ–ï–ù–û: —É—Å—Ç–æ–π—á–∏–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ ---------
def _extract_first_json_object(s: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—ã–π –ø–æ–ª–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç –∏–∑ —Å—Ç—Ä–æ–∫–∏, —É—á–∏—Ç—ã–≤–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä–æ–∫–∏."""
    in_str = False
    esc = False
    depth = 0
    start = None
    for i, ch in enumerate(s):
        if ch == '"' and not esc:
            in_str = not in_str
        if not in_str:
            if ch == '{':
                if depth == 0:
                    start = i
                depth += 1
            elif ch == '}':
                if depth > 0:
                    depth -= 1
                if depth == 0 and start is not None:
                    return s[start:i+1]
        esc = (ch == '\\' and not esc)
    # –ï—Å–ª–∏ JSON –Ω–µ –∑–∞–∫—Ä—ã—Ç, –Ω–æ –µ—Å—Ç—å –Ω–∞—á–∞–ª–æ - –≤–µ—Ä–Ω–µ–º –¥–æ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤)
    if start is not None and depth > 0:
        return s[start:]
    return ""

def parse_json_loose(raw):
    if isinstance(raw, dict):
        return raw
    if not isinstance(raw, str):
        raise ValueError("–û–∂–∏–¥–∞–ª–∞—Å—å —Å—Ç—Ä–æ–∫–∞ —Å JSON.")
    
    s = raw.strip()
    print(f"DEBUG: Original response length: {len(s)}")

    # –®–∞–≥ 1: –£–¥–∞–ª—è–µ–º markdown –±–ª–æ–∫–∏ –∫–æ–¥–∞ (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)
    s = re.sub(r'^```(?:json)?\s*', '', s, flags=re.MULTILINE)
    s = re.sub(r'```\s*$', '', s, flags=re.MULTILINE)
    s = s.strip()
    
    # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ HTML —Ç–µ–≥–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–µ –æ–±–µ—Ä—Ç–∫–∏
    s = re.sub(r'<[^>]+>', '', s)
    
    # –£–¥–∞–ª—è–µ–º –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (// ...) –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ (/* ... */)
    s = re.sub(r'//.*?$', '', s, flags=re.MULTILINE)
    s = re.sub(r'/\*.*?\*/', '', s, flags=re.DOTALL)
    
    # –®–∞–≥ 2: –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π }
    first_brace = s.find('{')
    last_brace = s.rfind('}')
    
    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ JSON —á–∞—Å—Ç—å
        s = s[first_brace:last_brace+1]
    elif first_brace != -1:
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞, –Ω–æ –Ω–µ—Ç –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π - –≤–æ–∑–º–æ–∂–Ω–æ JSON –æ–±—Ä–µ–∑–∞–Ω
        s = s[first_brace:]
    
    # –®–∞–≥ 3: –ü—ã—Ç–∞–µ–º—Å—è –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –≤—ã—Ä–µ–∑–∞—Ç—å –ø–µ—Ä–≤—ã–π –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç
    extracted = _extract_first_json_object(s)
    if extracted:
        try:
            result = json.loads(extracted)
            print("DEBUG: Successfully parsed using _extract_first_json_object")
            return result
        except json.JSONDecodeError as e:
            print(f"DEBUG: _extract_first_json_object parse failed: {e}")
            # –ï—Å–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π JSON –Ω–µ–ø–æ–ª–Ω—ã–π, –ø–æ–ø—Ä–æ–±—É–µ–º –µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
            if extracted.endswith('"') or extracted.endswith(','):
                # –í–æ–∑–º–æ–∂–Ω–æ, JSON –æ–±—Ä–µ–∑–∞–Ω
                pass
    
    # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π JSON
    open_braces = s.count('{')
    close_braces = s.count('}')
    open_brackets = s.count('[')
    close_brackets = s.count(']')
    
    if open_braces > close_braces or open_brackets > close_brackets:
        print(f"WARNING: JSON appears to be truncated. Open braces: {open_braces}, Close braces: {close_braces}, Open brackets: {open_brackets}, Close brackets: {close_brackets}")
    
    # –®–∞–≥ 5: –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å
    try:
        return json.loads(s)
    except json.JSONDecodeError as e:
        print(f"DEBUG: First parse attempt failed: {e}")
    
    # –®–∞–≥ 6: –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫
    try:
        start, end = s.find("{"), s.rfind("}")
        if start != -1 and end != -1 and end > start:
            json_part = s[start:end+1]
            print(f"DEBUG: Extracted JSON part length: {len(json_part)}")
            return json.loads(json_part)
    except json.JSONDecodeError as e:
        print(f"DEBUG: Second parse attempt failed: {e}")
    
    # –®–∞–≥ 7: –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã, —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
    # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã Unicode
    s2 = re.sub(r"[\u200b-\u200f\u202a-\u202e\u00a0]", "", s)
    s2 = s2.replace('\ufeff', '')  # BOM
    # –£–¥–∞–ª—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ \n, \r, \t –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ —Å—Ç—Ä–æ–∫–∞—Ö)
    s2 = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', s2)
    
    try:
        return json.loads(s2)
    except json.JSONDecodeError as e:
        print(f"DEBUG: Parse attempt with cleaned string failed: {e}")
    
    # –®–∞–≥ 8: –ü—Ä–æ–±—É–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å JSON —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
    s3 = s2
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∞—Ö
    in_string = False
    escape_next = False
    result_chars = []
    for i, char in enumerate(s3):
        if escape_next:
            result_chars.append(char)
            escape_next = False
            continue
        if char == '\\':
            result_chars.append(char)
            escape_next = True
            continue
        if char == '"':
            in_string = not in_string
            result_chars.append(char)
        elif char == '\n' and in_string:
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫
            result_chars.append('\\n')
        elif char == '\r' and in_string:
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫–∞—Ä–µ—Ç–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫
            result_chars.append('\\r')
        elif char == '\t' and in_string:
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±—É–ª—è—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫
            result_chars.append('\\t')
        else:
            result_chars.append(char)
    
    # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –∑–∞–∫—Ä—ã—Ç–∞, –∑–∞–∫—Ä—ã–≤–∞–µ–º –µ—ë
    if in_string:
        print("DEBUG: Found unterminated string, closing it")
        s3 = ''.join(result_chars) + '"'
    else:
        s3 = ''.join(result_chars)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–Ω–µ —Å—Ç—Ä–æ–∫
    s3_temp = s3
    in_str = False
    esc = False
    fixed_chars = []
    for char in s3_temp:
        if esc:
            fixed_chars.append(char)
            esc = False
            continue
        if char == '\\':
            fixed_chars.append(char)
            esc = True
            continue
        if char == '"':
            in_str = not in_str
            fixed_chars.append(char)
        elif char == "'" and not in_str:
            fixed_chars.append('"')
        else:
            fixed_chars.append(char)
    s3 = ''.join(fixed_chars)
    
    # –£–¥–∞–ª—è–µ–º –∑–∞–ø—è—Ç—ã–µ –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏/—Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
    s3 = re.sub(r',(\s*[}\]])', r'\1', s3)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∑–∞–∫–∞–≤—ã—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏: –∫–ª—é—á –±–µ–∑ –∫–∞–≤—ã—á–µ–∫ –ø–µ—Ä–µ–¥ : –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –∫–∞–≤—ã—á–∫–∞—Ö
    s3 = re.sub(r'(?<=[{\s,])([a-zA-Z_][a-zA-Z0-9_]*)(?=\s*:)', r'"\1"', s3)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–≤–æ–µ—Ç–æ—á–∏—è: –µ—Å–ª–∏ –µ—Å—Ç—å "key" –ø–µ—Ä–µ–¥ –∑–Ω–∞—á–µ–Ω–∏–µ–º –±–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º
    s3 = re.sub(r'"([^"]*?)"\s+(?=["\[{0-9true false])', r'"\1": ', s3)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \x –≤–º–µ—Å—Ç–æ \\x)
    # –ù–æ —Ç–æ–ª—å–∫–æ –≤–Ω–µ —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    # –≠—Ç–æ —Å–ª–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —à–∞–≥
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—Ä–µ–∑–∞–Ω–æ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "text –±–µ–∑ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π –∫–∞–≤—ã—á–∫–∏)
    # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –∏—Ö –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏
    if s3.count('"') % 2 != 0:
        # –ù–µ—á–µ—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–≤—ã—á–µ–∫ - –µ—Å—Ç—å –Ω–µ–∑–∞–∫—Ä—ã—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        print("DEBUG: Detected unclosed string, attempting to fix")
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É –±–µ–∑ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π
        last_open_quote = s3.rfind('"')
        if last_open_quote > 0:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π –∫–∞–≤—ã—á–∫–æ–π
            before_quote = s3[:last_open_quote]
            quote_count_before = before_quote.count('"')
            if quote_count_before % 2 == 0:
                # –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è –∫–∞–≤—ã—á–∫–∞ –±–µ–∑ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º –µ—ë –ø–µ—Ä–µ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–æ–π
                last_brace = s3.rfind('}')
                if last_brace > last_open_quote:
                    s3 = s3[:last_brace] + '"' + s3[last_brace:]
    
    # –£–¥–∞–ª—è–µ–º –∑–∞–ø—è—Ç—ã–µ –≤ –∫–æ–Ω—Ü–µ –æ–±—ä–µ–∫—Ç–æ–≤/–º–∞—Å—Å–∏–≤–æ–≤ (–ø–æ–≤—Ç–æ—Ä–Ω–æ, –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏)
    s3 = re.sub(r',(\s*[}\]])', r'\1', s3)
    
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≤–µ—Ä—à–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π JSON, –¥–æ–±–∞–≤–ª—è—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏
    open_braces = s3.count('{')
    close_braces = s3.count('}')
    open_brackets = s3.count('[')
    close_brackets = s3.count(']')
    
    if open_braces > close_braces:
        missing_braces = open_braces - close_braces
        print(f"DEBUG: Attempting to complete truncated JSON by adding {missing_braces} closing braces")
        # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ –∏ –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Å–∫–æ–±–æ–∫
        s3 = s3.rstrip(',\n\r\t ') + '}' * missing_braces
    
    if open_brackets > close_brackets:
        missing_brackets = open_brackets - close_brackets
        print(f"DEBUG: Attempting to complete truncated arrays by adding {missing_brackets} closing brackets")
        s3 = s3.rstrip(',\n\r\t ') + ']' * missing_brackets
    
    try:
        return json.loads(s3)
    except json.JSONDecodeError as e2:
        print(f"DEBUG: Even after fixes, parsing failed: {e2}")
        error_pos = getattr(e2, 'pos', 0)
        if error_pos > 0:
            start_pos = max(0, error_pos - 200)
            end_pos = min(len(s3), error_pos + 200)
            print(f"DEBUG: Problematic JSON around error position {error_pos}:")
            print(f"DEBUG: Context: {repr(s3[start_pos:end_pos])}")
        
        # –ü—Ä–æ–±—É–µ–º –µ—â–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏
        try:
            last_brace = s3.rfind('}')
            if last_brace > 0:
                s4 = s3[:last_brace+1]
                result = json.loads(s4)
                print("DEBUG: Successfully parsed by truncating to last closing brace")
                return result
        except json.JSONDecodeError as e3:
            print(f"DEBUG: Truncation to last brace failed: {e3}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞: –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏ –∏ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏
        print("DEBUG: Attempting final fix: closing all unclosed brackets")
        s5 = s3
        try:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏ –∏ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏
            brace_depth = 0
            bracket_depth = 0
            in_str = False
            esc = False
            for char in s5:
                if esc:
                    esc = False
                    continue
                if char == '\\':
                    esc = True
                    continue
                if char == '"':
                    in_str = not in_str
                    continue
                if not in_str:
                    if char == '{':
                        brace_depth += 1
                    elif char == '}':
                        brace_depth = max(0, brace_depth - 1)
                    elif char == '[':
                        bracket_depth += 1
                    elif char == ']':
                        bracket_depth = max(0, bracket_depth - 1)
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å–∫–æ–±–∫–∏ –∏ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏
            s5 = s5.rstrip(',\n\r\t ')
            s5 += ']' * bracket_depth + '}' * brace_depth
            result = json.loads(s5)
            print(f"DEBUG: Successfully parsed after closing {bracket_depth} brackets and {brace_depth} braces")
            return result
        except json.JSONDecodeError as e4:
            print(f"DEBUG: Final fix also failed: {e4}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π JSON –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                with open("debug_json_error.txt", "w", encoding="utf-8") as f:
                    f.write(f"First parse error: {e if 'e' in locals() else 'N/A'}\n")
                    f.write(f"Fixed error (e2): {e2}\n")
                    f.write(f"Final error (e4): {e4}\n")
                    f.write(f"Error position: {error_pos if 'error_pos' in locals() else 'N/A'}\n")
                    f.write(f"Original response length: {len(s)}\n")
                    f.write(f"Cleaned string length: {len(s2)}\n")
                    f.write(f"Fixed string length: {len(s3)}\n")
                    f.write(f"\nOriginal problematic JSON (first 5000 chars):\n{s2[:5000]}\n\n")
                    f.write(f"After fixes (first 5000 chars):\n{s3[:5000]}\n\n")
                    if 's5' in locals() and s5:
                        f.write(f"Attempted final fix (first 5000 chars):\n{s5[:5000]}")
            except Exception as save_err:
                print(f"DEBUG: Failed to save debug file: {save_err}")
            
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {e4}. –î–µ—Ç–∞–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ debug_json_error.txt. –í–æ–∑–º–æ–∂–Ω–æ, –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏.")

def _extract_technologies_from_vacancy(vacancy_text: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏"""
    import re
    
    # –û–±—â–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
    tech_patterns = [
        r'\b(?:Python|Java|JavaScript|TypeScript|C\#|C\+\+|PHP|Ruby|Go|Rust|Kotlin|Swift)\b',
        r'\b(?:React|Angular|Vue|Django|Flask|Spring|Laravel|Express|Node\.js)\b',
        r'\b(?:PostgreSQL|MySQL|MongoDB|Redis|Elasticsearch|Oracle|SQL Server)\b',
        r'\b(?:Docker|Kubernetes|AWS|Azure|GCP|Jenkins|GitLab|GitHub)\b',
        r'\b(?:Linux|Windows|MacOS|Ubuntu|CentOS)\b',
        r'\b(?:Git|SVN|Mercurial)\b',
        r'\b(?:REST|GraphQL|API|JSON|XML|SOAP)\b',
        r'\b(?:HTML|CSS|SASS|LESS|Bootstrap|Tailwind)\b',
        r'\b(?:Webpack|Vite|Babel|ESLint|Prettier)\b',
        r'\b(?:Terraform|Ansible|Puppet|Chef)\b',
        r'\b(?:Prometheus|Grafana|ELK|Splunk)\b',
        r'\b(?:Kafka|RabbitMQ|ActiveMQ)\b',
        r'\b(?:Hadoop|Spark|Airflow|Greenplum)\b'
    ]
    
    technologies = set()
    for pattern in tech_patterns:
        matches = re.findall(pattern, vacancy_text, re.IGNORECASE)
        technologies.update(matches)
    
    return list(technologies)

def _highlight_technologies_in_text(doc: Document, technologies: list):
    """–í—ã–¥–µ–ª—è–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    if not technologies:
        return
        
    import re
    
    for paragraph in doc.paragraphs:
        if not paragraph.text.strip():
            continue
            
        original_text = paragraph.text
        has_matches = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤ —ç—Ç–æ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ
        for tech in technologies:
            if re.search(r'\b' + re.escape(tech) + r'\b', original_text, re.IGNORECASE):
                has_matches = True
                break
        
        if not has_matches:
            continue
            
        # –û—á–∏—â–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º
        paragraph.clear()
        
        remaining_text = original_text
        while remaining_text:
            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            earliest_match = None
            earliest_pos = len(remaining_text)
            matched_tech = None
            
            for tech in technologies:
                match = re.search(r'\b' + re.escape(tech) + r'\b', remaining_text, re.IGNORECASE)
                if match and match.start() < earliest_pos:
                    earliest_pos = match.start()
                    earliest_match = match
                    matched_tech = tech
            
            if earliest_match is None:
                # –ù–µ—Ç –±–æ–ª—å—à–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–π —Ç–µ–∫—Å—Ç
                paragraph.add_run(remaining_text)
                break
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –¥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if earliest_pos > 0:
                paragraph.add_run(remaining_text[:earliest_pos])
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∂–∏—Ä–Ω—ã–º
            bold_run = paragraph.add_run(earliest_match.group())
            bold_run.bold = True
            
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ—Å—Ç–∞–≤—à–∏–º—Å—è —Ç–µ–∫—Å—Ç–æ–º
            remaining_text = remaining_text[earliest_match.end():]

def build_prompt_simple(candidate_text: str, vacancy_text: str) -> str:
    return f"""
–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –∫–∞–∫–æ–≥–æ-–ª–∏–±–æ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ –æ–±—ä–µ–∫—Ç–∞. –ù–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –∫–æ–¥–∞, –º–∞—Ä–∫–¥–∞—É–Ω–∞, –ø–æ—è—Å–Ω–µ–Ω–∏–π.
–¢–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç {{"config":{{...}}, "content":{{...}}}}.
White Label: –Ω–µ –≤–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ email. –°–æ—Ö—Ä–∞–Ω–∏ –í–°–Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.
–ï—Å–ª–∏ –Ω–µ—Ç Summary ‚Äî —Å–æ–∑–¥–∞–π 3‚Äì5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –û–ø—Ä–µ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –ø–æ –≤–∞–∫–∞–Ω—Å–∏–∏.
–ì–†–ï–ô–î: –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –∫–∞–∫ Senior, Middle –∏–ª–∏ Junior –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã.
–ì–†–ê–ñ–î–ê–ù–°–¢–í–û: –æ–ø—Ä–µ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞—Ü–∏–∏ –∏ —É–∫–∞–∂–∏ –∫–∞–∫ –†–§ (–¥–ª—è –†–æ—Å—Å–∏–∏/–ú–æ—Å–∫–≤—ã), –†–ë (–¥–ª—è –ë–µ–ª–∞—Ä—É—Å–∏/–ú–∏–Ω—Å–∫–∞), –∏–ª–∏ –≤–æ–∑—å–º–∏ –∏–∑ —Ä–µ–∑—é–º–µ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ.
–ü–†–û–ï–ö–¢–´ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã: –Ω–∞–π–¥–∏ –≤—Å–µ –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ —Å–ø—Ä—è—Ç–∞–Ω—ã –≤ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—è—Ö/–û–±–æ –º–Ω–µ.
–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø (–û —Å–µ–±–µ): —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π –∫–æ–º–ø–∞–∫—Ç–Ω–æ, –æ–±—ä–µ–¥–∏–Ω—è–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫.

‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –¢–†–ï–ë–û–í–ê–ù–ò–ï –ö JSON (–ü–†–û–í–ï–†–Ø–ô –ü–ï–†–ï–î –í–´–í–û–î–û–ú):
1. –¢–û–õ–¨–ö–û –æ–¥–∏–Ω JSON-–æ–±—ä–µ–∫—Ç: {{ ... }}
2. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
3. –í—Å–µ –∫–ª—é—á–∏ –≤ –¥–≤–æ–π–Ω—ã—Ö –∫–∞–≤—ã—á–∫–∞—Ö: "key"
4. –í—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤ –¥–≤–æ–π–Ω—ã—Ö –∫–∞–≤—ã—á–∫–∞—Ö: "value"
5. –ö–ê–ñ–î–´–ô –∫–ª—é—á –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏–º–µ–µ—Ç ":  " (–¥–≤–æ–µ—Ç–æ—á–∏–µ —Å –ø—Ä–æ–±–µ–ª–æ–º). –ü—Ä–∞–≤–∏–ª—å–Ω–æ: "key": "value". –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: "key" "value"
6. –ï—Å–ª–∏ –∫–∞–≤—ã—á–∫–∞ –≤ –∑–Ω–∞—á–µ–Ω–∏–∏ ‚Äî —ç–∫—Ä–∞–Ω–∏—Ä—É–π: \"
7. –ù–µ—Ç –∑–∞–ø—è—Ç—ã—Ö –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ–¥ }} –∏–ª–∏ ]]
8. –ü–æ—Å—á–∏—Ç–∞–π: {{ –¥–æ–ª–∂–Ω—ã —Ä–∞–≤–Ω—è—Ç—å—Å—è }}, [ –¥–æ–ª–∂–Ω—ã —Ä–∞–≤–Ω—è—Ç—å—Å—è ]
9. –ï–°–õ–ò –ß–¢–û-–¢–û –ù–ï –¢–ê–ö ‚Äî –ü–ï–†–ï–î–ï–õ–ê–ô JSON –ø–æ–ª–Ω–æ—Å—Ç—å—é –ü–ï–†–ï–î –í–´–í–û–î–û–ú

–ó–ê–ü–û–ú–ù–ò: –∫–∞–∂–¥–æ–º—É –∫–ª—é—á—É –ù–ï–û–ë–•–û–î–ò–ú–û –¥–≤–æ–µ—Ç–æ—á–∏–µ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∫–∞–≤—ã—á–∫–∏ "key": 
–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–ó–ê–ü–†–ï–©–ï–ù–´):
- "key" value  (–Ω–µ—Ç –¥–≤–æ–µ—Ç–æ—á–∏—è)
- "key"value   (–Ω–µ—Ç –¥–≤–æ–µ—Ç–æ—á–∏—è)
- "key" "value" (–Ω–µ—Ç –¥–≤–æ–µ—Ç–æ—á–∏—è)
–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä:
- "key": "value"  ‚úì
- "key": 123  ‚úì
- "key": true  ‚úì
- "key": ["item1", "item2"]  ‚úì

–°—Ö–µ–º–∞:
{{
 "config": {{
   "output_format": "docx",
   "font_family": "Times New Roman",
   "font_size_main": 12,
   "font_size_headings": 14,
   "color_headings": "#1F4E79",
   "language": "ru",
   "sections": [
     "–§–ò–û","–†–ï–ó–Æ–ú–ï","–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è",
     "–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏","–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã","–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ","–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è","–ü—Ä–æ–µ–∫—Ç—ã"
   ],
   "white_label": true,
   "exclude_contacts": true,
   "exclude_email": true
 }},
 "content": {{
   "fio": {{"full_name":"","location":"","citizenship":"","birth_date":""}},
   "position_grade":"", "grade":"", "summary":"",
   "skills": {{}},
   "experience":[{{"company":"","position":"","period":"","responsibilities":[],"technologies":[],"achievements":[]}}],
   "education":[{{"institution":"","degree":"","years":"","details":""}}],
   "extra": "—Ç–µ–∫—Å—Ç –û —Å–µ–±–µ –æ–¥–Ω–∏–º –∞–±–∑–∞—Ü–µ–º –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑",
   "projects":[{{"title":"","role":"","period":"","description":"","technologies":[],"results":""}}]
 }}
}}

–ò–°–•–û–î–ù–û–ï –†–ï–ó–Æ–ú–ï:
{candidate_text}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –í–ê–ö–ê–ù–°–ò–ò:
{vacancy_text}
"""

async def generate_payload_once(
                          candidate_text: str,
                          vacancy_text: str,
                          model : genai.GenerativeModel) -> dict:
    
    prompt = build_prompt_simple(candidate_text, vacancy_text)
    resp = await model.generate_content_async(
        prompt,
        generation_config=genai.types.GenerationConfig(
            temperature=0.1,
            max_output_tokens=20480,  # –ú–∞–∫—Å–∏–º—É–º –¥–ª—è Gemini-2.5 (–±—ã–ª–æ 12288)
            response_mime_type="application/json"  # <-- –ñ—ë—Å—Ç–∫–∏–π JSON-—Ä–µ–∂–∏–º
        )
    )
    raw = _extract_text_from_gemini_response(resp)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
    finish_reason = None
    try:
        if resp.candidates:
            finish_reason = getattr(resp.candidates[0], "finish_reason", None)
    except Exception:
        pass
    
    # finish_reason=2 –æ–∑–Ω–∞—á–∞–µ—Ç MAX_TOKENS ‚Äî –æ—Ç–≤–µ—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω
    if finish_reason == 2:
        print(f"WARNING: –û—Ç–≤–µ—Ç –æ—Ç Gemini –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω (MAX_TOKENS). –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π JSON.")
        if not raw:
            raise ValueError(f"–û—Ç–≤–µ—Ç –æ—Ç Gemini –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω (MAX_TOKENS) –∏ –ø—É—Å—Ç. –£–≤–µ–ª–∏—á—å—Ç–µ max_output_tokens –∏–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
        # –ü—Ä–æ–¥–æ–ª–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥, –¥–∞–∂–µ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω
    elif not raw:
        safety = None
        try:
            if resp.candidates:
                safety = getattr(resp.candidates[0], "safety_ratings", None)
        except Exception:
            pass
        raise ValueError(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Gemini. finish_reason={finish_reason}, safety={safety}")
    data = parse_json_loose(raw)
    if not isinstance(data, dict) or "config" not in data or "content" not in data:
        raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ JSON —Å –∫–ª—é—á–∞–º–∏ {config, content}.")
    cfg = data.setdefault("config", {})
    if "sections" in cfg and "–ü—Ä–æ–µ–∫—Ç—ã" not in cfg["sections"]:
        cfg["sections"].append("–ü—Ä–æ–µ–∫—Ç—ã")
    return data

async def create_white_label_resume(candidate_text: str,
                                   vacancy_text: str,
                                   model : genai.GenerativeModel,
                                   utochnenie=None,
                                   username = ""):
    
    payload = await generate_payload_once(candidate_text, vacancy_text, model)
    filename = render_resume_docx(payload, vacancy_text, utochnenie=utochnenie, username = username)
    return filename

#===== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π, –ø–æ–¥—Å—Ç–∞–≤—å API –∫–ª—é—á –∏ —Ç–µ–∫—Å—Ç—ã) =====

import os
from dotenv import load_dotenv
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if __name__ == "__main__":
    candidate = '''
      
Contact
 Apt 321, 31, Veshnyakovskaya
 str., Moscow, 111538, Russian
 Federation.
 7-916-685-9607 (Mobile)
 sergts@mail.ru
 www.linkedin.com/in/sergts
 (LinkedIn)
 github.com/itboss2 (Portfolio)
 Top Skills
 System Architects
 Telecommunications Billing
 WebSphere ESB
 Languages
 English - C1 Certified (Full
 Professional)
 Bulgarian (Elementary)
 Russian - C2 Certifed (Native or
 Bilingual)
 Byelorussian - C2 Certified (Full
 Professional)
 Polish (Limited Working)
 German (Elementary)
 Ukrainian (Elementary)
 Certifications
 Palo Alto Networks Accredited
 Configuration Engineer (ACE) 
PAN-OS 6.1
 Cisco Network Support Engineer
 VMware Technical Sales
 Professional 5 for 6 competencies:
 Infrastructure Virtualization, Desktop
 Virtualization, Business Continuity,
 Virtualization of Business Critical
 Applications, Infrastructure as a
 Service, Management
 IBM Certified Database Associate 
DB2 10.1 Fundamentals
 Honors-Awards
 3rd prize winner of Russia Huawei
 2017 Channel Partner Skill
 Competition
 Sergey Tsuprikov, Tech
 Advisor, IBM DS, LSSBB,
 MCP,Mentor
 IT architect and project manager with 10+ years of experience in
 Big Data, Data Science, enterprise software design area | Banking
 | Telecom | FinTech | Data Lake | DWH | BI | Data Driven | Data
 Management | Data Quality
 Moscow, Moscow City, Russia
 Summary
 12+ years of experience as an IT architect for enterprise software
 (like OEBSl) design, complicated cross-platform data migration and
 integration, distributed data centers turn-key design.
 7+ years of experience as a project manager for projects up to 15M
 USD. 
7+ years of experience as a business and/or system analyst, mostly
 in the banking area. 
5+ years of experience as an out-staff instructor (PMI PMBOK,
 PRINCE2, ITIL, IBM solutions).
 10+ years of experience as a subject matter expert (SME) in banking
 and finance, sales and marketing, logistics, manufacturing, retail,
 FMCG. 
5+ years of successful daily work face-to-face and remote with
 Indian banking analysts from Oracle Corp. 
My mobile phone: 7-916-685-9607 (9:00-22:00 GMT+3, Moscow),
 Telegram: @sergts1 .
 My private email - sergts@mail.ru (please, use bossit@gmail.com
 only in case of troubles with sending to sergts@mail.ru).
 I have 30+ years of experience in IT. Have received 120+ worldwide
 IT certificates after exams (i.e. 70+ technical from IBM): Program
 & Portfolio Management Expert (#35), Project Management
 Expert (PME) #000412, IBM Data Science Professional, VMware
 Certified Professional 4 (#63533), EMCTAe, Six Sigma Black Belt
 Professional, MCP, etc. 
 Page 1 of 9
  ...
'''
    vacancy = '''
    BD-10128 (https://t.me/omega_vacancy_bot?start=3093_BD-10128)
üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: 08.10.2025 12:13
... (—É–∫–æ—Ä–æ—á–µ–Ω–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞) ...
'''
    # # –ü—Ä–∏–º–µ—Ä —É—Ç–æ—á–Ω–µ–Ω–∏–π:
    # utochnenie = [
    #     "–≤–ª–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–º —è–∑—ã–∫–æ–º (—É—Ä–æ–≤–µ–Ω—å –Ω–µ —É–∫–∞–∑–∞–Ω)",
    #     "–æ–ø—ã—Ç —Å Greenplum (–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Å—Ç–µ–∫)",
    #     "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è AWS (–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –≥–æ–¥)"
    # ]
    # fn = create_white_label_resume_once(GEMINI_API_KEY, candidate, vacancy, utochnenie=utochnenie)
    # print("–ì–æ—Ç–æ–≤–æ:", fn)
